<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
            displayMath: [['$$','$$'], ['\\[','\\]']],
            CommonHTML: { linebreaks: { automatic: true } },
            "HTML-CSS": { linebreaks: { automatic: true } },
            SVG: { linebreaks: { automatic: true } }
        });
    </script>
    <style>
        /* Simple CSS Reset */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        /* Basic Styling */
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            text-align: center;
            padding: 30px 0;
            border-bottom: 1px solid #e0e0e0;
            margin-bottom: 30px;
        }

        h1 {
            font-size: 2.5rem;
            margin-bottom: 20px;
            color: #2c3e50;
            width: 100%;
        }

        .author-info {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 15px;
            margin-bottom: 20px;
        }

        .author a {
            color: #3498db;
            text-decoration: none;
            font-weight: 600;
            font-size: 2rem;
        }

        .author a:hover {
            text-decoration: underline;
        }

        .date {
            font-size: 2rem;
            color: #7f8c8d;
        }

        section {
            margin-bottom: 40px;
        }

        h2 {
            color: #3498db;
            margin-bottom: 15px;
            padding-bottom: 5px;
            border-bottom: 1px solid #e0e0e0;
        }

        h3 {
            color: #2980b9;
            margin-bottom: 10px;
        }

        p {
            margin-bottom: 15px;
        }

        ul, ol {
            margin-left: 20px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Add these CSS styles to your existing stylesheet */
        .video-comparison {
            background-color: #f5f5f5;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .video-comparison h3 {
            color: #2c3e50;
            text-align: center;
            margin-bottom: 15px;
        }

        .video-container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            justify-content: center;
        }

        .video-item {
            flex: 1;
            min-width: 300px;
            max-width: 450px;
            text-align: center;
            background-color: white;
            padding: 15px;
            border-radius: 6px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .video-item h4 {
            color: #3498db;
            margin-bottom: 10px;
        }

        .video-item video {
            width: 100%;
            border-radius: 4px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
        }

        .video-caption {
            font-style: italic;
            color: #666;
            margin-top: 10px;
            font-size: 0.9rem;
        }

        /* Responsive adjustments */
        @media (max-width: 768px) {
            .video-container {
                flex-direction: column;
                align-items: center;
            }
            
            .video-item {
                width: 100%;
            }
        }

        /* Add these CSS styles to your existing stylesheet */
        .diagram-container {
            background-color: #f0f7fa;
            border-radius: 8px;
            padding: 20px;
            margin: 25px 0;
            text-align: center;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            border: 1px solid #d0e8f2;
        }

        .mmaudio-diagram {
            max-width: 90%;
            height: auto;
            border-radius: 6px;
            box-shadow: 0 3px 6px rgba(0, 0, 0, 0.15);
            margin: 0 auto 15px auto;
            display: block;
        }

        .diagram-caption {
            font-size: 0.95rem;
            color: #2c3e50;
            line-height: 1.5;
            max-width: 85%;
            margin: 0 auto;
            text-align: center;
            font-style: italic;
        }

        .code-block {
            background-color: #272822;
            border: 1px solid #1e1f1c;
            border-radius: 6px;
            padding: 20px;
            margin: 15px 0;
            overflow-x: auto;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        pre {
            font-family: 'JetBrains Mono', 'Fira Code', 'Courier New', Courier, monospace;
            white-space: pre-wrap;
            color: #f8f8f2;
            line-height: 1.5;
            font-size: 0.95rem;
        }

        /* Monokai-inspired Syntax Highlighting */
        .code-keyword {
            color: #f92672; /* pink */
        }

        .code-string {
            color: #e6db74; /* yellow */
        }

        .code-comment {
            color: #75715e; /* grey-brown */
            font-style: italic;
        }

        .code-function {
            color: #a6e22e; /* green */
        }

        .code-class {
            color: #66d9ef; /* blue */
            font-style: italic;
        }

        .code-builtin {
            color: #66d9ef; /* blue */
        }

        .code-number {
            color: #ae81ff; /* purple */
        }

        #experiments {
            margin-bottom: 40px;
        }
        
        #experiments h3 {
            color: #2980b9;
            margin-bottom: 15px;
        }
        
        #experiments {
            margin-bottom: 40px;
        }
        
        #experiments h3 {
            color: #2980b9;
            margin-bottom: 15px;
        }
        
        #experiments h4 {
            color: #3498db;
            margin-bottom: 10px;
            font-size: 1.1rem;
        }
        
        .results-container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin-bottom: 20px;
        }
        
        .result-item {
            flex: 1;
            min-width: 300px;
            background-color: #f5f9fc;
            padding: 15px;
            border-radius: 6px;
            border-left: 4px solid #3498db;
            margin-bottom: 20px;
        }
        
        .table-container {
            margin: 15px 0;
            overflow-x: auto;
        }
        
        .results-table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.9rem;
            box-shadow: 0 2px 3px rgba(0,0,0,0.1);
        }
        
        .results-table caption {
            font-style: italic;
            padding: 8px;
            text-align: left;
            font-size: 0.8rem;
            color: #555;
        }
        
        .results-table th {
            background-color: #e6f2f9;
            border: 1px solid #c5d9e8;
            padding: 8px;
            text-align: center;
        }
        
        .results-table td {
            border: 1px solid #ddd;
            padding: 6px;
            text-align: center;
        }
        
        .table-note {
            font-size: 0.8rem;
            color: #666;
            margin-top: 5px;
            font-style: italic;
        }
        
        .highlight-row {
            background-color: #f0f7fa;
        }
        
        .highlight-row td {
            font-weight: 600;
        }
        
        .visualization {
            background-color: #f8f8f8;
            border: 1px solid #e0e0e0;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
            text-align: center;
        }
        
        .spectrogram-img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .visualization-caption {
            font-style: italic;
            color: #666;
            margin-top: 10px;
            font-size: 0.9rem;
        }
        
        .ablation-studies {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(450px, 1fr));
            gap: 20px;
            margin-bottom: 20px;
        }
        
        .ablation-item {
            background-color: #f0f7fa;
            padding: 15px;
            border-radius: 6px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }
        
        .ablation-item h4 {
            border-bottom: 1px solid #d0e8f2;
            padding-bottom: 5px;
            margin-bottom: 10px;
        }
        
        .ablation-conclusion {
            background-color: #ebf5fb;
            padding: 15px;
            border-left: 4px solid #3498db;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        @media (max-width: 768px) {
            .results-container {
                flex-direction: column;
            }
            
            .ablation-studies {
                grid-template-columns: 1fr;
            }
            
            .result-item, .ablation-item {
                width: 100%;
            }
            
            .results-table {
                font-size: 0.8rem;
            }
            
            .results-table th, .results-table td {
                padding: 4px;
            }
        }

        .reflection-item {
            background-color: #ebf5fb;
            padding: 15px;
            border-left: 4px solid #3498db;
            margin-bottom: 15px;
        }

        .references ol {
            padding-left: 20px;
        }

        .references li {
            margin-bottom: 10px;
        }

        footer {
            text-align: center;
            padding: 20px 0;
            margin-top: 30px;
            border-top: 1px solid #e0e0e0;
            color: #7f8c8d;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            
            h1 {
                font-size: 2rem;
            }
            
            .code-block {
                padding: 10px;
            }
        }

        /* Print Styles */
        @media print {
            body {
                background-color: white;
                color: black;
            }
            
            .code-block, .visualization, .reflection-item {
                break-inside: avoid;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis</h1>
        <div class="author-info">
            <div class="author">By <a href="https://github.com/karan-kumawat17" target="_blank">Karan Kumawat</a> (220150005)</div>
            <div class="date">May 7, 2025</div>
        </div>
    </header>
    
    <main>
        <section id="motivation">
            <h2>Motivation</h2>
            <p>
                Ever faced the problem of having a video but no audio? Generating realistic, synchronized audio from video is a challenging task that has significant implications in various fields, including film production, gaming, and virtual reality. The ability to synthesize high-quality audio that matches the visual content can enhance user experience and immersion. Existing methods either rely on limited video-audio datasets or bolt "control modules" onto text-to-audio models, which can lead to suboptimal results. This is where the concept of multimodal joint training comes into play.
            </p>
            <div class="video-comparison">
                <h3>Compare the difference:</h3>
                <div class="video-container">
                    <div class="video-item">
                        <h4>Video without Sound</h4>
                        <video width="400" height="300" controls muted>
                            <source src="videos/without_sound.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p class="video-caption">The same video without audio - notice what's missing</p>
                    </div>

                    <div class="video-item">
                        <h4>Video with Sound</h4>
                        <video width="400" height="300" controls>
                            <source src="videos/with_sound.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p class="video-caption">Experience the full audiovisual experience</p>
                    </div>                    
                </div>
            </div>
            <p>
                <b>The Innovation:</b> MMAudio proposes a novel approach for Foley i.e. to video-to-audio synthesis by leveraging multimodal joint training. By integrating video, text and audio data into a single training framework, the model learns to generate high-quality audio that is not only synchronized with the visual content but also semantically relevant. This approach allows for better generalization across different domains and improves the overall quality of the synthesized audio.
            </p>
            <div class="diagram-container">
                <img src="images/pipeline.png" alt="MMAudio Architecture Diagram" class="mmaudio-diagram">
                <p class="diagram-caption">
                    MMAudio performing multimodal joint training with high-quality, abundant audio-text data which enables effective data scaling. 
                    At inference, MMAudio generates conditions-aligned audio with video and/or text guidance.
                </p>
            </div>
            <p>
                <b>Relevance Today:</b>
                <ul>
                    <li>With the rise of content creation platforms and the increasing demand for high-quality audio-visual content, the need for efficient video-to-audio synthesis methods has never been greater.</li>
                    <li>Multimodal learning is a hot topic in AI research, and MMAudio represents a significant step forward in this field.</li>
                    <li>The ability to generate realistic audio from video can revolutionize industries such as gaming, film, and virtual reality.</li>
                </ul>
            </p>
        </section>
        
        <section id="historical-perspective">
            <h2> The Evolution of Multimodal Learning for Audio-Visual Synthesis </h2>
            <p>
                Multimodal learning has evolved significantly over the past few years, driven by advancements in deep learning and the availability of large-scale datasets. Early approaches focused on simple feature concatenation or shallow fusion techniques, which often failed to capture the complex relationships between different modalities.
            </p>

            <p>
                <b>Semantic Alignment</b> has been a foundational concept in multimodal learning, focusing on establishing meaningful connections between different modalities at a conceptual level:
                <ul>
                    <li>Early approaches relied on paired audio-visual data trained with either generative objectives ([2, 6]) or contrastive objectives ([42]), teaching models to understand cross-modal relationships.</li>
                    <li>The field advanced by incorporating audio-text paired training alongside audio-visual training, creating more robust semantic understanding that could transfer between modalities.</li>
                    <li>Innovations like ImageBind [11] and LanguageBind [30] demonstrated how joint training could create shared semantic spaces where conceptually similar content clusters together regardless of original modality.</li>
                    <li>Modern approaches now focus on omnidirectional feature sharing, allowing information to flow freely between all modalities rather than in predetermined directions.</li>
                </ul>
            </p>
            <p>
                <b>Temporal Alignment</b> represents another critical dimension in multimodal learning, particularly for time-based media like video and audio:
                <ul>
                    <li>Traditional methods relied on handcrafted proxy features such as audio onsets [32, 33], energy patterns [17, 20], or root-mean-square of waveforms [4, 34] to synchronize modalities.</li>
                    <li>Contemporary approaches have shifted toward learning alignment directly from deep feature embeddings, with models like Synchformer [19] enabling more nuanced interpretation of temporal relationships.</li>
                    <li>Recent innovations in positional embeddings have significantly improved synchronization capabilities - some approaches scale up low-frequency (visual) embeddings while others subsample high-frequency (audio) embeddings to achieve better alignment.</li>
                    <li>Higher frame-rate processing (evolving from 8 FPS to 24 FPS) has dramatically improved audio-visual synchronization quality, particularly for precise timing events.</li>
                </ul>
            </p>
            <p>
                <b>Multimodal Conditionding</b> approaches have transformed how models integrate information from multiple sources:
                <ul>
                    <li>The conventional approach involved adding "control modules" to inject visual features into pretrained text-to-audio networks [13, 17, 20, 34], but this increased parameter counts and created architectural inefficiencies.</li>
                    <li>This modular approach faced limitations as the text modality remained fixed during video-to-audio training, forcing the video modality to adapt to text semantics rather than allowing mutual adaptation.</li>
                    <li>Modern architectures now train all modalities simultaneously in joint frameworks, creating more coherent semantic spaces and enabling more natural cross-modal generation.</li>
                    <li>Alternative techniques like Seeing-and-Hearing [35] attempted alignment without joint training by performing gradient ascent on alignment scores at test time, but these approaches often resulted in lower quality and temporal misalignment compared to jointly-trained models.</li>
                </ul>
            </p>
            <p>
                This historical trajectory shows a clear progression from isolated single-modality approaches toward more integrated systems that can understand and generate content across the full spectrum of human sensory experience, with each advancement building upon previous innovations to create increasingly sophisticated and capable models.
            </p>
        </section>
        
        <section id="learnings">
            <h2>Key Learnings</h2>
            <p>
                Through my exploration of multimodal learning on video-audio synthesis, I've gained several important insights:
            </p>
            <ul>
                <li>
                    <p><strong>Problem of Foley Sound Synthesis:</strong> Creating convincing, high-quality ambient sound effects for video (Foley) is complex. It requires not only generating realistic audio but ensuring it is <b>semantically aligned</b> and <b>temporally aligned</b>.</p>
                </li>
                <li>
                    <p><strong>Multimodal Joint Training Approach:</strong> A central learing is the effectiveness of MMAudio's novel multimodal joint training framework. Instead of just training on video-audio data or adapting pre-trained models, MMAudio trains on video, audio and text <i>jointly</i> in a single network from scratch.</p>
                </li>
                <li>
                    <p><strong>Attention Mechanisms:</strong> Cross-attention layers allow models to focus on relevant parts of one modality while processing information from another, enabling more nuanced understanding across modalities.</p>
                </li>
                <li>
                    <p><strong>Conditional Flow Matching:</strong> Conditional Flow Matching is used for generative modeling. For more details, readers can refer to [63]. For a quick overview, at test time, to generate a sample, we randomly draw noise $x_0 \sim \mathcal{N}(0,I)$ and use an Ordinary Differential Equation(ODE) solver to numerically integrate from time $t = 0$ to time $t = 1$ following a learned time-dependent conditional velocity vector field $v_theta(t, \mathbf{C}, x):[0,1] \times \mathbb{R}^C \times \mathbb{R}^d \to \mathbb{R}^d$, where $t$ is the timestep, $\mathbf{C}$ is the condition (e.g., video and text), and $x$ is a point in the vector field. We represent the velocity vector field via a deep net parameterized by $\theta$.</p>
                    <p>At training time, we find $\theta$ by considering the conditional flow matching objective</p>
                    <p style="text-indent: 2em">$\mathbb{E}_{t,q(x_0),q(x_1,\mathbf{C})}||v_\theta(t, \mathbf{C}, x_t) - u(x_t|x_0, x_1)||^2$,</p>
                    <p>where $t \in [0, 1]$, $q(x_0)$ is the standard normal distribution, and $q(x_1, \mathbf{C})$ samples from the training data. Further,</p>
                    <p style="text-indent: 2em"> $x_t = tx_1 + (1-t)x_0$</p>
                    <p>defines a linear interpolation path between noise and data, and</p>
                    <p style="text-indent: 2em;">$u(x_t|x_0, x_1) = x_1 - x_0$</p>
                    <p>denotes its corresponding flow velocity at $x_t$.</p>
                </li>
                <li>
                    <p><strong>Audio Encoding:</strong> For computational efficiency, we model generative process in a latent space. For this, we first transform audio waveforms via Short-Time Fourier Transform (STFT) and extract mel spectrograms, which are then encoded by a pretrained VAE into latents $x_1$. During testing, the generated latents are decoded by the VAE into spectrograms which are then vocoded by a pretrained vocoder into audio waveforms.</p>
                </li>
                <li>
                    <p><strong>Overview:</strong> The below figure illustrates the network architecture. MMAudio consists of a series of ($N_1$) <b> multimodal transformer</b> blocks with visual/text/audio branches, followed by a series of ($N_2$) audio-only transformer blocks. For scynchrony, they devised a <b>conditional synchronization module</b> that extracts and integrates into the generation process for temporal alignment.</p>
                </li>
                <li>
                    <p><strong>Multimodal Transformer:</strong> Our desire is to model the interactions between video, audio and text modalities. For this purpose, we largely adopt the MM-DiT block design from SD3[8] and introduce two new components for temporal alignment: <b>aligned RoPE positional embeddings</b> for aligning sequences of different frame rates and <b>1D convolutional MLPs</b> for capturing local temporal structure. Note, they also included a self-attention. This design help us to build a deeper network with the same parameter count and compute without sacrificing multimodality.</p>
                </li>
                <li>
                    <p><strong>Representations:</strong> All the features are represented in one-dimensional tokens. Absolute positional encoding is not used which allows us to generalize for different duration of videos. The visual features $F_v$ (one token per frame, at 8 fps) and text features $F_t$ (77 tokens) are extracted from CLIP as 1024d features. The audio latents $x$ are in the VAE latent space at 31.25 fps as 20d latents by default. The synchronization features $F_{syn}$ are extracted with Synchformer at 24 fps as 768d features. Except text tokens, all other follow the same temporal ordering.</p>
                </li>
                <li>
                    <p><strong>Joint Attention:</strong> These tokens from different modalities communicate via joint attention. We use the query, key and value representations from all modalities and apply scaled dot product attention. They noticed that joint attention alone does not capture temporal alignment.</p>
                </li>
                <li>
                    <p><strong>Aligned RoPE position embedding:</strong> For audio-visual synchrony, precise temporal alignment is crucial. They applied RoPE embeddings on the queries and keys in the visual and audio streams before joint attention. It is not applied to text stream. For aligning the fps, they scaled the frequencies of positional embeddings in the visual stream proportionally. They also introduced additional synchronization module for better synchrony.</p>
                </li>
                <li>
                    <p><strong>ConvMLP:</strong> For better capturing of local temperature structure, ConvMLPs are used for streams. They used 1D convolutions (kernel size = 3 and padding = 1).</p>
                </li>
                <li>
                    <p><strong>Global Conditionding:</strong> This injects global features into the network through scales and biases ind adaptive layer normalization layers (adaLN). First, they computed a global conditioning vector $c_g \in \mathbb{R}^{1 \times h}$ shared across all transformer blocks from Fourier encoding of flow timestep. Each layer modulates its input $y \in \mathbb{R}^(L \times h)$ ($L$ is the sequence length) as:</p>
                    <p style="text-indent: 2em;">
                        $$\text{adaLN}_y(y, c_y) = \text{LayerNorm}(y) \cdot \mathbf{1}\mathbf{W}_{\gamma}(c_g) + \mathbf{1}\mathbf{W}_{\beta}(c_g)$$
                    </p>
                    <p>Here, $\mathbf{W}_{\gamma}, \mathbf{W}_{\beta}$ are MLPs, and <b>1</b> is a $L \times 1$ all-ones matrix.</p>
                </li>
                <li>
                    <p><strong>Effective Data Scaling:</strong> Joint training enables effective data scaling. This is a crucial insight for training as the datasets alone is limited by data scarcity and expense. By jointly training on larger-scale, readily available audio-text data (like <b>WavCaps</b>, which is much bigger than <b>VGGSound</b>) effectively scaling the data. This approach helps learn a unified semantic space across modalities</p>
                </li>
                <li>
                    <p><strong>Conditional synchronization Module:</strong> Token-level conditioning is developed to improve audio-visual synchrony. The cross-modality attention layers aggregate features via a soft distribution, which tampers precision. To deal with this, high frame rate (24fps) are extraced ($F_{syn}$) from input video.</p>
                </li>
                <li>
                    <p><strong>Single-Modality Performance:</strong> Surprisingly, MMAudio's multimodal approach also achieves <b>competitive performance in text-to-audio generation</b>, even comparable to models specifically designed for that task. So, we indeed do have rich semantic feature space.</p>
                </li>
                <li>
                    <p><strong>Addressing Data Overlaping Issues:</strong> The existence of data overlaps between common datasets used in video-to-audio research and states they have carefully removed overlapping samples from the training data for fair evaluation. This is important for unbiased results.</p>
                </li>
                <li>
                    <p><strong>Limitations:</strong> There are some limitations to the model. It generates unintelligible mumbles when prompted to generate human speech. The theory which is believed behind this is human speech is more complex.</p>
                </li>
            </ul>
            <div class="diagram-container">
                <img src="images/architecture.png" alt="MMAudio Architecture Diagram" class="mmaudio-diagram">
                <p class="diagram-caption">
                    The architecture of MMAudio. It consists of a series of multimodal transformer blocks with visual/text/audio branches, followed by a series of audio-only transformer blocks. For synchrony, they devised a conditional synchronization module that extracts and integrates into the generation process for temporal alignment.
                    <br>Note: The figure is a conceptual representation and may not reflect the exact architecture used in the paper.
                </p>
            </div>
        </section>
        
        <section id="code-and-demos">
            <h2>Code and Demonstrations</h2>
            <h3>Feature Extraction Pipeline</h3>
            <div class="code-block">
                <pre>
<span class="code-comment"># Simplified audio feature extraction</span>
<span class="code-keyword">class</span> <span class="code-class">AudioFeatureExtractor</span>:
    <span class="code-keyword">def</span> __init__(self, model_name):
        self.clip_model = <span class="code-function">load_clip_model</span>()
        self.vae = <span class="code-function">load_vae_model</span>()
        self.mel_converter = <span class="code-function">MelConverter</span>()

    <span class="code-keyword">def</span> <span class="code-function">process</span>(self, waveform):
        mel = self.mel_converter(waveform)
        latents = self.vae.encode(mel)
        text_features = self.clip_model.encode_text(latents)
        <span class="code-keyword">return</span> latents, text_features

<span class="code-comment"># Simplified video feature extraction</span>
<span class="code-keyword">class</span> <span class="code-class">VideoFeatureExtractor</span>:
    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self):
        self.synchformer = <span class="code-function">load_synchronization_model</span>()
        self.clip_encoder = <span class="code-function">load_video_encoder</span>()
        
    <span class="code-keyword">def</span> <span class="code-function">process</span>(self, video_frames):
        clip_features = self.clip_encoder(video_frames)
        sync_features = self.synchformer(video_frames)
        <span class="code-keyword">return</span> clip_features, sync_features
                </pre>
            </div>

            <h3>Multimodal Fusion Architecture</h3>
            <div class="code-block">
                <pre>
<span class="code-keyword">class</span> <span class="code-class">MultimodalTransformer</span>(nn.Module):
    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self):
        super().<span class="code-function">__init__</span>()
        self.cross_attn = nn.MultiheadAttention(embed_dim=1024, num_heads=16)
        self.temporal_conv = nn.Conv1d(1024, 1024, kernel_size=3)
        self.adaLN = AdaptiveLayerNorm(1024)
        
    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, video_feats, audio_feats, text_feats):
        <span class="code-comment"># Joint attention across modalities</span>
        combined = self.cross_attn(
            query=audio_feats,
            key=torch.cat([video_feats, text_feats], dim=1),
            value=torch.cat([video_feats, text_feats], dim=1)
        )
        <span class="code-comment"># Temporal processing</span>
        aligned = self.temporal_conv(combined)
        <span class="code-comment"># Conditional normalization</span>
        output = self.adaLN(aligned)
        <span class="code-keyword">return</span> output</pre>
    </div>

            <h3>Training and Evaluation</h3>
            <div class="code-block">
                <pre>
<span class="code-keyword">def</span> <span class="code-function">train_step</span>(batch, models):
    <span class="code-comment"># Extract multimodal features</span>
    audio_feats = models.audio_extractor(batch['waveform'])
    video_feats = models.video_extractor(batch['video_frames'])
    text_feats = models.text_encoder(batch['text'])
                    
    <span class="code-comment"># Multimodal fusion</span>
    fused_feats = models.fuser(video_feats, audio_feats, text_feats)
                    
    <span class="code-comment"># Flow matching objective</span>
    noise = torch.randn_like(fused_feats)
    t = torch.rand(fused_feats.size(0))
    x_t = t[:,None,None]*fused_feats + (1-t[:,None,None])*noise
    pred_velocity = models.velocity_net(x_t, t)
    loss = F.mse_loss(pred_velocity, fused_feats - noise)
                    
    <span class="code-keyword">return</span> loss</pre>
    </div>

    <h3>Demo</h3>
    <p>Check out the demo of MMAudio in action. This was done in Colab environment:</p>
    <div class="code-block">
        <pre>
<span class="code-comment"># Colab demo for MMAudio</span>
<span class="code-keyword">!nvidia-smi</span>
<span class="code-comment"># Check GPU availability</span>
<span class="code-keyword">import</span> torch
<span class="code-keyword">if</span> torch.cuda.is_available():
    <span class="code-keyword">print</span>(<span class="code-string">'GPU is available'</span>)
    <span class="code-keyword">device</span> = <span class="code-string">'cuda'</span>
<span class="code-keyword">else</span>:
    <span class="code-keyword">print</span>(<span class="code-string">'GPU not available, using CPU'</span>)    
    <span class="code-keyword">device</span> = <span class="code-string">'cpu'</span>
    
<span class="code-comment"># Install dependencies</span>
<span class="code-keyword">!pip install torch torchvision torchaudio transformers</span>
<span class="code-keyword">!git clone https://github.com/hkchengrex/MMAudio.git</span>
<span class="code-keyword">%cd</span> MMAudio
<span class="code-keyword">!pip</span> install -e .
    
<span class="code-comment"># Example data:</span>
<span class="code-keyword">%cd</span> /content/MMAudio
<span class="code-keyword">!curl</span> <span class="code-string">https://i.imgur.com/8xHJTzI.mp4</span> -O
<span class="code-keyword">from</span> <span class="code-class">IPython.display</span> <span class="code-keyword">import</span> HTML
<span class="code-keyword">from</span> <span class="code-class">base64</span> <span class="code-keyword">import</span> b64encode
<span class="code-keyword">data_url</span> = <span class="code-string">"data:video/mp4;base64,"</span> + b64encode(<span class="code-keyword">open</span>(<span class="code-string">'video.mp4'</span>, <span class="code-string">'rb'</span>).read()).decode()
HTML(<span class="code-string">'&lt;video width="400" height="300" controls&gt;&lt;source src="'</span> + data_url + <span class="code-string">'" type="video/mp4"&gt;&lt;/video&gt;'</span>)    
    
<span class="code-comment"># Load the model</span>
<span class="code-keyword">!python</span> demo.py --duration=10 --video=video.mp4 --prompt <span class="code-string">"waves and seagulls"</span>
<span class="code-keyword">data_url</span> = <span class="code-string">"data:video/mp4;base64,"</span> + b64encode(<span class="code-keyword">open</span>(<span class="code-string">'./output/video.mp4'</span>, <span class="code-string">'rb'</span>).read()).decode()
HTML(<span class="code-string">'&lt;video width="400" height="300" controls&gt;&lt;source src="'</span> + data_url + <span class="code-string">'" type="video/mp4"&gt;&lt;/video&gt;'</span>)    
        </pre>
    </div>
        </section>
        
        <section id="Experiments">
            <h2>Experiments</h2>
    
            <section id="metrics">
                <h3>Metrics</h3>
                <p>
                    We evaluate generation quality across four dimensions:
                </p>
                <ul>
                    <li>
                        <strong>Distribution matching:</strong> Measures similarity between ground-truth and generated audio using Fréchet Distance (FD) with PaSST, PANNs, and VGGish embeddings, and Kullback-Leibler (KL) distance with PANNs and PaSST classifiers.
                    </li>
                    <li>
                        <strong>Audio quality:</strong> Assessed using Inception Score with PANNs classifier.
                    </li>
                    <li>
                        <strong>Semantic alignment:</strong> Measured via ImageBind cosine similarity (IB-score) between visual and audio features.
                    </li>
                    <li>
                        <strong>Temporal alignment:</strong> Quantified by Synchformer's desynchronization score (DeSync), measuring audio-visual misalignment in seconds.
                    </li>
                </ul>
            </section>
            
            <section id="main-results">
                <h3>Main Results</h3>
                
                <div class="results-container">
                    <div class="result-item">
                        <h4>Video-to-audio</h4>
                        <p>
                            Our smallest model (157M) outperforms prior methods on the VGGSound test set (~15K videos) across most metrics while being computationally efficient. Larger models show improved FD<sub>PaSST</sub> and IB-scores, though with diminishing returns likely due to data quality limitations. We evaluate 8-second generations following Wang et al.
                        </p>
                        
                        <div class="table-container">
                            <table class="results-table">
                                <caption>Table 2. Onset accuracy, average precision (AP), and F1-score on Greatest Hits, with DeSync on VGGSound for reference.</caption>
                                <thead>
                                    <tr>
                                        <th>Method</th>
                                        <th>Acc. ↑</th>
                                        <th>AP ↑</th>
                                        <th>F1↑</th>
                                        <th>DeSync↓</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Frieren [67]</td>
                                        <td>0.6949</td>
                                        <td>0.7846</td>
                                        <td>0.6550</td>
                                        <td>0.851</td>
                                    </tr>
                                    <tr>
                                        <td>V-AURA [65]</td>
                                        <td>0.5852</td>
                                        <td>0.8567</td>
                                        <td>0.6441</td>
                                        <td>0.654</td>
                                    </tr>
                                    <tr>
                                        <td>FoleyCrafter [73]</td>
                                        <td>0.4533</td>
                                        <td>0.6939</td>
                                        <td>0.4319</td>
                                        <td>1.225</td>
                                    </tr>
                                    <tr>
                                        <td>Seeing&Hearing [69]</td>
                                        <td>0.1156</td>
                                        <td>0.8342</td>
                                        <td>0.1591</td>
                                        <td>1.204</td>
                                    </tr>
                                    <tr class="highlight-row">
                                        <td>MMAudio-S-16kHz</td>
                                        <td><strong>0.7637</strong></td>
                                        <td><strong>0.9010</strong></td>
                                        <td><strong>0.7928</strong></td>
                                        <td>0.483</td>
                                    </tr>
                                    <tr>
                                        <td>MMAudio-S-44.1kHz</td>
                                        <td>0.7150</td>
                                        <td><strong>0.9097</strong></td>
                                        <td>0.7666</td>
                                        <td>0.444</td>
                                    </tr>
                                    <tr>
                                        <td>MMAudio-M-44.1kHz</td>
                                        <td>0.7226</td>
                                        <td>0.9054</td>
                                        <td>0.7620</td>
                                        <td>0.443</td>
                                    </tr>
                                    <tr>
                                        <td>MMAudio-L-44.1kHz</td>
                                        <td>0.7158</td>
                                        <td>0.9064</td>
                                        <td>0.7535</td>
                                        <td><strong>0.442</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                    
                    <div class="result-item">
                        <h4>Text-to-audio</h4>
                        <p>
                            Without fine-tuning, our multimodal framework demonstrates state-of-the-art semantic alignment (CLAP) and audio quality (IS) on the AudioCaps test set, despite not being primarily designed for this task.
                        </p>
                        
                        <div class="table-container">
                            <table class="results-table">
                                <caption>Table 3. Text-to-audio results on the AudioCaps test set. For a fair comparison, we follow the evaluation protocol of [12] and transcribe all baselines directly from [12], who have reproduced those results using officially released checkpoints under the same evaluation protocol.</caption>
                                <thead>
                                    <tr>
                                        <th>Method</th>
                                        <th>Params</th>
                                        <th>FD<sub>PANNs</sub>↓</th>
                                        <th>FD<sub>VGG</sub>↓</th>
                                        <th>IS↑</th>
                                        <th>CLAP↑</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>AudioLDM 2-L [39]</td>
                                        <td>712M</td>
                                        <td>32.50</td>
                                        <td>5.11</td>
                                        <td>8.54</td>
                                        <td>0.212</td>
                                    </tr>
                                    <tr>
                                        <td>TANGO [1]</td>
                                        <td>866M</td>
                                        <td>26.13</td>
                                        <td>1.87</td>
                                        <td>8.23</td>
                                        <td>0.185</td>
                                    </tr>
                                    <tr>
                                        <td>TANGO 2 [43]</td>
                                        <td>866M</td>
                                        <td>19.77</td>
                                        <td>2.74</td>
                                        <td>8.45</td>
                                        <td>0.264</td>
                                    </tr>
                                    <tr>
                                        <td>Make-An-Audio [16]</td>
                                        <td>453M</td>
                                        <td>27.93</td>
                                        <td>2.59</td>
                                        <td>7.44</td>
                                        <td>0.207</td>
                                    </tr>
                                    <tr>
                                        <td>Make-An-Audio 2 [15]</td>
                                        <td>937M</td>
                                        <td>15.34</td>
                                        <td>1.27</td>
                                        <td>9.58</td>
                                        <td>0.251</td>
                                    </tr>
                                    <tr>
                                        <td>GenAU-Large [12]</td>
                                        <td>1.25B</td>
                                        <td>16.51</td>
                                        <td><strong>1.21</strong></td>
                                        <td>11.75</td>
                                        <td>0.285</td>
                                    </tr>
                                    <tr>
                                        <td>MMAudio-S-16kHz</td>
                                        <td>157M</td>
                                        <td><strong>14.42</strong></td>
                                        <td>2.98</td>
                                        <td>11.36</td>
                                        <td>0.282</td>
                                    </tr>
                                    <tr>
                                        <td>MMAudio-S-44.1kHz</td>
                                        <td>157M</td>
                                        <td>15.26</td>
                                        <td>2.74</td>
                                        <td>11.32</td>
                                        <td>0.331</td>
                                    </tr>
                                    <tr>
                                        <td>MMAudio-M-44.1kHz</td>
                                        <td>621M</td>
                                        <td><strong>14.38</strong></td>
                                        <td>4.07</td>
                                        <td>12.02</td>
                                        <td>0.351</td>
                                    </tr>
                                    <tr class="highlight-row">
                                        <td>MMAudio-L-44.1kHz</td>
                                        <td>1.03B</td>
                                        <td>15.04</td>
                                        <td>4.03</td>
                                        <td><strong>12.08</strong></td>
                                        <td><strong>0.348</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </div>
                
                <div class="visualization">
                    <img src="images/spectrogram.png" alt="Visualization of spectrograms from different models" class="spectrogram-img">
                    <p class="visualization-caption">Figure 3. We visualize the spectrograms of generated audio (by prior works and our method) and the ground-truth. Note our method generates the audio effects most closely aligned to the ground-truth, while other methods often generate sounds not explained by the visual input and not present in the ground-truth.</p>
                </div>
            </section>
            
            <section id="ablations">
                <h3>Ablations</h3>
                <p>
                    All ablations use our small-16kHz model evaluated on the VGGSound test set.
                </p>
                
                <div class="ablation-studies">
                    <div class="ablation-item">
                        <h4>Cross-modal alignment</h4>
                        <p>
                            Joint multimodal training benefits from:
                        </p>
                        <ol>
                            <li>Incorporating text modality, creating a unified feature space</li>
                            <li>Including uncaptioned audio data, which improves natural sound distribution learning</li>
                            <li>Training on large multimodal datasets rather than only utilizing class labels</li>
                        </ol>
                        
                        <div class="table-container">
                            <table class="results-table">
                                <caption>Table 4. Results when we vary the training modalities. A: Audio, V: Video, T: Text.</caption>
                                <thead>
                                    <tr>
                                        <th>Training modalities</th>
                                        <th>FD<sub>PaSST</sub>↓</th>
                                        <th>IS↑</th>
                                        <th>IB-score↑</th>
                                        <th>DeSync↓</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="highlight-row">
                                        <td>AVT+AT</td>
                                        <td><strong>70.19</strong></td>
                                        <td><strong>14.44</strong></td>
                                        <td><strong>29.13</strong></td>
                                        <td><strong>0.483</strong></td>
                                    </tr>
                                    <tr>
                                        <td>AV+AT</td>
                                        <td>72.77</td>
                                        <td>12.88</td>
                                        <td>28.10</td>
                                        <td>0.502</td>
                                    </tr>
                                    <tr>
                                        <td>AVT+A</td>
                                        <td>71.01</td>
                                        <td>14.30</td>
                                        <td>28.72</td>
                                        <td>0.496</td>
                                    </tr>
                                    <tr>
                                        <td>AV+A</td>
                                        <td>77.38</td>
                                        <td>12.53</td>
                                        <td>27.98</td>
                                        <td>0.562</td>
                                    </tr>
                                    <tr>
                                        <td>AV</td>
                                        <td>77.27</td>
                                        <td>12.69</td>
                                        <td>28.10</td>
                                        <td>0.502</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <p class="table-note">In the second and third rows, we mask away the text token in either audio-visual data or audio-text data. In the last two rows, we do not use any audio-text data.</p>
                    </div>
                    
                    <div class="ablation-item">
                        <h4>Multimodal data</h4>
                        <p>
                            Increasing the amount of audio-text training data improves distribution matching, semantic alignment, and temporal alignment, with diminishing returns at larger scales.
                        </p>
                        
                        <div class="table-container">
                            <table class="results-table">
                                <caption>Table 5. Results when we vary the amount of multimodal training data.</caption>
                                <thead>
                                    <tr>
                                        <th>% audio-text data</th>
                                        <th>FD<sub>PaSST</sub>↓</th>
                                        <th>IS↑</th>
                                        <th>IB-score↑</th>
                                        <th>DeSync↓</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="highlight-row">
                                        <td>100%</td>
                                        <td><strong>70.19</strong></td>
                                        <td>14.44</td>
                                        <td><strong>29.13</strong></td>
                                        <td><strong>0.483</strong></td>
                                    </tr>
                                    <tr>
                                        <td>50%</td>
                                        <td>71.03</td>
                                        <td><strong>14.62</strong></td>
                                        <td>29.11</td>
                                        <td>0.489</td>
                                    </tr>
                                    <tr>
                                        <td>25%</td>
                                        <td>71.67</td>
                                        <td>14.41</td>
                                        <td>28.75</td>
                                        <td>0.505</td>
                                    </tr>
                                    <tr>
                                        <td>10%</td>
                                        <td>79.21</td>
                                        <td>13.55</td>
                                        <td>27.47</td>
                                        <td>0.514</td>
                                    </tr>
                                    <tr>
                                        <td>None</td>
                                        <td>77.38</td>
                                        <td>12.53</td>
                                        <td>27.98</td>
                                        <td>0.562</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <p class="table-note">For the first four rows, we sample audio-visual and audio-text data at a 1:1 ratio during training. For the last row, only audio-visual data is used.</p>
                    </div>
                </div>
                
                <div class="ablation-studies">
                    <div class="ablation-item">
                        <h4>Conditional synchronization module</h4>
                        <p>
                            Our approach outperforms alternatives in temporal alignment compared to incorporating synchronization features into the visual branch or omitting them entirely.
                        </p>
                        
                        <div class="table-container">
                            <table class="results-table">
                                <caption>Table 6. Results when we use synchronization features differently.</caption>
                                <thead>
                                    <tr>
                                        <th>Variant</th>
                                        <th>FD<sub>PaSST</sub>↓</th>
                                        <th>IS↑</th>
                                        <th>IB-score↑</th>
                                        <th>DeSync↓</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="highlight-row">
                                        <td>With sync module</td>
                                        <td>70.19</td>
                                        <td>14.44</td>
                                        <td>29.13</td>
                                        <td><strong>0.483</strong></td>
                                    </tr>
                                    <tr>
                                        <td>Sum sync with visual</td>
                                        <td>73.59</td>
                                        <td><strong>16.70</strong></td>
                                        <td>28.65</td>
                                        <td>0.490</td>
                                    </tr>
                                    <tr>
                                        <td>No sync features</td>
                                        <td><strong>69.33</strong></td>
                                        <td>15.05</td>
                                        <td><strong>29.31</strong></td>
                                        <td>0.973</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                    
                    <div class="ablation-item">
                        <h4>RoPE embeddings</h4>
                        <p>
                            Aligned RoPE formulation improves audio-visual synchrony compared to both no RoPE embeddings and non-aligned variants.
                        </p>
                        
                        <div class="table-container">
                            <table class="results-table">
                                <caption>Part of Table 6. Results when we use RoPE embeddings differently.</caption>
                                <thead>
                                    <tr>
                                        <th>Variant</th>
                                        <th>FD<sub>PaSST</sub>↓</th>
                                        <th>IS↑</th>
                                        <th>IB-score↑</th>
                                        <th>DeSync↓</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="highlight-row">
                                        <td>Aligned RoPE</td>
                                        <td><strong>70.19</strong></td>
                                        <td>14.44</td>
                                        <td>29.13</td>
                                        <td><strong>0.483</strong></td>
                                    </tr>
                                    <tr>
                                        <td>No RoPE</td>
                                        <td>70.24</td>
                                        <td><strong>14.54</strong></td>
                                        <td><strong>29.23</strong></td>
                                        <td>0.509</td>
                                    </tr>
                                    <tr>
                                        <td>Non-aligned RoPE</td>
                                        <td>70.25</td>
                                        <td><strong>14.54</strong></td>
                                        <td>29.25</td>
                                        <td>0.496</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </div>
                
                <div class="ablation-studies">
                    <div class="ablation-item">
                        <h4>ConvMLP</h4>
                        <p>
                            Outperforms standard MLP in capturing local temporal structure, particularly for synchronization.
                        </p>
                        
                        <div class="table-container">
                            <table class="results-table">
                                <caption>Part of Table 7. Results when we vary the MLP architecture.</caption>
                                <thead>
                                    <tr>
                                        <th>Variant</th>
                                        <th>FD<sub>PaSST</sub>↓</th>
                                        <th>IS↑</th>
                                        <th>IB-score↑</th>
                                        <th>DeSync↓</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="highlight-row">
                                        <td>ConvMLP</td>
                                        <td><strong>70.19</strong></td>
                                        <td><strong>14.44</strong></td>
                                        <td><strong>29.13</strong></td>
                                        <td><strong>0.483</strong></td>
                                    </tr>
                                    <tr>
                                        <td>MLP</td>
                                        <td>73.84</td>
                                        <td>13.01</td>
                                        <td>28.99</td>
                                        <td>0.533</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                    
                    <div class="ablation-item">
                        <h4>Architecture ratio</h4>
                        <p>
                            Our default assignment of multimodal (N<sub>1</sub>=4) and single-modal (N<sub>2</sub>=8) transformer blocks balances performance and parameter efficiency.
                        </p>
                        
                        <div class="table-container">
                            <table class="results-table">
                                <caption>Part of Table 7. Results when we vary the ratio between multi-/single-modality transformer blocks.</caption>
                                <thead>
                                    <tr>
                                        <th>Variant</th>
                                        <th>FD<sub>PaSST</sub>↓</th>
                                        <th>IS↑</th>
                                        <th>IB-score↑</th>
                                        <th>DeSync↓</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="highlight-row">
                                        <td>N<sub>1</sub> = 4, N<sub>2</sub> = 8</td>
                                        <td><strong>70.19</strong></td>
                                        <td>14.44</td>
                                        <td>29.13</td>
                                        <td><strong>0.483</strong></td>
                                    </tr>
                                    <tr>
                                        <td>N<sub>1</sub> = 2, N<sub>2</sub> = 13</td>
                                        <td>70.33</td>
                                        <td><strong>15.18</strong></td>
                                        <td><strong>29.39</strong></td>
                                        <td>0.487</td>
                                    </tr>
                                    <tr>
                                        <td>N<sub>1</sub> = 6, N<sub>2</sub> = 3</td>
                                        <td>72.53</td>
                                        <td>13.75</td>
                                        <td>29.06</td>
                                        <td>0.509</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </div>
                
                <p class="ablation-conclusion">
                    The ablation studies confirm that our design choices are essential for achieving high-quality video-to-audio synthesis. Joint multimodal training with text data, aligned RoPE embeddings, our conditional synchronization module, and ConvMLP all contribute significantly to the model's performance, especially for temporal alignment and audio quality.
                </p>
            </section>
        </section>
        <section id="reflections">
            <h2>Reflections</h2>
    
            <div class="reflection-item">
                <h3>What Surprised Me?</h3>
                <ol>
                    <li>
                        <h4>Multimodal Training Doesn't Compromise Single-Modality Performance</h4>
                        <p>The paper reveals that training on <em>both</em> video-audio <em>and</em> text-audio data doesn't dilute the model's ability to excel in text-to-audio tasks. This challenges the assumption that multimodal models must trade off specialization for versatility. For instance, MMAudio's CLAP score (text-audio alignment) rivals dedicated text-to-audio models like <em>AudioLDM 2</em>, suggesting that cross-modal training enriches semantic understanding.</p>
                    </li>
                    <li>
                        <h4>Efficiency Without Sacrificing Quality</h4>
                        <p>Despite its compact size (157M parameters), MMAudio outperforms larger models (e.g., <em>FoleyCrafter</em>: 1.2B params) in synchronization and audio quality. The use of flow matching—a less common alternative to diffusion models—proves unexpectedly effective for fast, stable generation.</p>
                    </li>
                    <li>
                        <h4>Synchformer's Dual Role</h4>
                        <p>The repurposing of <em>Synchformer</em>—a model originally designed to <em>detect</em> desynchronization—to <em>improve</em> synchronization is a clever inversion. Its high-FPS features (24 fps) enable frame-level precision, a stark contrast to prior methods relying on handcrafted proxies like energy curves.</p>
                    </li>
                </ol>
            </div>
            
            <div class="reflection-item">
                <h3>Scope for Improvement</h3>
                <ol>
                    <li>
                        <h4>Speech Synthesis: The Elephant in the Room</h4>
                        <p>While MMAudio excels at ambient sounds and Foley effects, its failure to generate intelligible speech highlights a critical gap. Human speech involves layers of complexity (phonetics, prosody, language structure) absent in non-vocal audio. Future work could:</p>
                        <ul>
                            <li>Integrate <strong>speech-specific modules</strong> (e.g., phoneme aligners, tone predictors)</li>
                            <li>Leverage <strong>speech-text-video datasets</strong> (e.g., talking-head videos with transcripts) to bridge this gap</li>
                        </ul>
                    </li>
                    <li>
                        <h4>Data Diversity and Bias Mitigation</h4>
                        <p>MMAudio trains on automated captions from WavCaps and class-labeled VGGSound data, which may embed biases (e.g., overrepresentation of common sounds). Curating <strong>balanced, ethically sourced datasets</strong>—with annotations for rare or culturally specific sounds—could improve fairness and generalization.</p>
                    </li>
                    <li>
                        <h4>Long-Form Temporal Consistency</h4>
                        <p>The model processes 8–10s clips, but real-world applications (e.g., films, podcasts) require coherence over minutes. Expanding the context window with <strong>memory-augmented transformers</strong> or hierarchical modeling could address this.</p>
                    </li>
                    <li>
                        <h4>Real-Time Applications</h4>
                        <p>While MMAudio is fast (1.23s for 8s audio), true real-time synthesis (e.g., for live streaming) demands sub-second latency. Optimizing the vocoder or exploring <strong>latent space streaming</strong> could unlock this.</p>
                    </li>
                    <li>
                        <h4>Integration with Video Generation Models</h4>
                        <p>Combining MMAudio with video generators (e.g., Sora, Stable Video Diffusion) could enable end-to-end <strong>audiovisual synthesis</strong>, where AI generates synchronized sight and sound from text prompts.</p>
                    </li>
                    <li>
                        <h4>Ethical Safeguards</h4>
                        <p>The paper sidesteps discussions on misuse (e.g., deepfake audio for misinformation). Future iterations should include <strong>watermarking</strong> or <strong>detection mechanisms</strong> to mitigate risks.</p>
                    </li>
                </ol>
            </div>
            
            <div class="reflection-item">
                <h3>Final Thoughts</h3>
                <p>MMAudio's limitations are not failures but signposts for progress. Its inability to handle speech underscores the need for <strong>specialized submodules</strong> in multimodal frameworks, while its reliance on existing datasets calls for more inclusive data practices. By addressing these gaps, the next generation of multimodal models could achieve human-like audiovisual understanding—a leap toward AI that truly "hears" and "sees" in tandem.</p>
            </div>
        </section>

        <section class="acknowledgements">
            <h2>Acknowledgements</h2>
            <p>This work is supported in part by Sony.
                ASis supported by NSF grants 2008387, 2045586, 2106825,
                and NIFA award 2020-67021-32799. I sincerely thank
                Kazuki Shimada and Zhi Zhong for their helpful feedback
                on this manuscript</p>
        </section>
        
        <section id="references" class="references">
            <h2>References</h2>
            <ol>
                <li>Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, and Yuki Mitsufuji. (2025). <em>MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis</em>. arXiv preprint arXiv:2412.15322. <a href="https://arxiv.org/abs/2412.15322">https://arxiv.org/abs/2412.15322</a></li>
                <li>Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. VGGSound: A large-scale audio-visual dataset. In <em>ICASSP</em>, 2020.</li>
                <li>Ziyang Chen, Prem Seetharaman, Bryan Russell, Oriol Nieto, David Bourgin, Andrew Owens, and Justin Salamon. Video-guided foley sound generation with multimodal controls. <em>arXiv</em>, 2024.</li>
                <li>Yoonjin Chung, Junwon Lee, and Juhan Nam. T-foley: A controllable waveform-domain diffusion model for temporal event-guided foley sound synthesis. In <em>ICASSP</em>. IEEE, 2024.</li>
                <li>Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset. In <em>ICASSP</em>. IEEE, 2020.</li>
                <li>Yuexi Du, Ziyang Chen, Justin Salamon, Bryan Russell, and Andrew Owens. Conditional generation of audio from video via foley analogies. In <em>CVPR</em>, 2023.</li>
                <li>Benjamin Elizalde, Soham Deshmukh, and Huaming Wang. Natural language supervision for general-purpose audio representations. In <em>ICASSP</em>, 2024.</li>
                <li>Patrick Esser et al. Scaling rectified flow transformers for high-resolution image synthesis. In <em>ICML</em>, 2024.</li>
                <li>Jort F Gemmeke et al. Audio set: An ontology and human-labeled dataset for audio events. In <em>ICASSP</em>. IEEE, 2017.</li>
                <li>Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using instruction tuned llm and latent diffusion model. <em>arXiv preprint arXiv:2304.13731</em>, 2023.</li>
                <li>Rohit Girdhar et al. Imagebind: One embedding space to bind them all. In <em>CVPR</em>, 2023.</li>
                <li>Moayed Haji-Ali et al. Taming data and transformers for audio generation. <em>arXiv preprint arXiv:2406.19388</em>, 2024.</li>
                <li>Jiawei Huang et al. Make-an-audio 2: Temporal-enhanced text-to-audio generation. <em>arXiv preprint arXiv:2305.18474</em>, 2023.</li>
                <li>Rongjie Huang et al. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. In <em>ICML</em>, 2023.</li>
                <li>Vladimir Iashin and Esa Rahtu. Taming visually guided sound generation. In <em>BMVC</em>, 2021.</li>
                <li>Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronization from sparse cues. In <em>ICASSP</em>. IEEE, 2024.</li>
                <li>Yujin Jeong, Yunji Kim, Sanghyuk Chun, and Jiyoung Lee. Read, watch and scream! sound generation from text and video. <em>arXiv preprint arXiv:2407.05551</em>, 2024.</li>
                <li>Chris Dongjoo Kim et al. AudioCaps: Generating captions for audios in the wild. In <em>NAACL-HLT</em>, 2019.</li>
                <li>Qiuqiang Kong et al. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. <em>TASLP</em>, 2020.</li>
                <li>Andrew Owens et al. Visually indicated sounds. In <em>CVPR</em>, 2016.</li>
                <li>Alec Radford et al. Learning transferable visual models from natural language supervision. In <em>ICLR</em>, 2021.</li>
                <li>Yong Ren et al. STA-V2A: Video-to-audio generation with semantic and temporal alignment. <em>arXiv preprint arXiv:2409.08601</em>, 2024.</li>
                <li>Ludan Ruan et al. MM-Diffusion: Learning multi-modal diffusion models for joint audio and video generation. In <em>CVPR</em>, 2023.</li>
                <li>Tim Salimans et al. Improved techniques for training gans. In <em>NeurIPS</em>, 2016.</li>
                <li>Zineng Tang et al. Any-to-any generation via composable diffusion. In <em>NeurIPS</em>, 2024.</li>
                <li>Ashish Vaswani et al. Attention is all you need. In <em>NeurIPS</em>, 2017.</li>
                <li>Yongqi Wang et al. Frieren: Efficient video-to-audio generation with rectified flow matching. In <em>NeurIPS</em>, 2024.</li>
                <li>Yazhou Xing et al. Seeing and hearing: Open-domain visual-audio generation with diffusion latent aligners. In <em>CVPR</em>, 2024.</li>
                <li>Yiming Zhang et al. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. <em>arXiv preprint arXiv:2407.01494</em>, 2024.</li>
                <li>Bin Zhu et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. In <em>ICLR</em>, 2024.</li>
                <li>Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao.
                    Diff-foley: Synchronized video-to-audio synthesis with latent
                    diffusion models. In <em>NeurIPS</em>, 2024.</li>
                <li>Yong Ren, Chenxing Li, Manjie Xu, Wei Liang, Yu Gu,
                    Rilin Chen, and Dong Yu. Sta-v2a: Video-to-audio gen
                   eration with semantic and temporal alignment. arXiv preprint
                    <em>arXiv:2409.08601</em>, 2024</li>
                <li>Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing,
                    Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter:
                    Bring silent videos to life with lifelike and synchronized
                    sounds. arXiv preprint <em>arXiv:2407.01494</em>, 2024.</li>
                <li> Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and
                    Qifeng Chen. Seeing and hearing: Open-domain visual-audio
                    generation with diffusion latent aligners. In <em>CVPR</em>, 2024.</li>
            </ol>
        </section>
    </main>
    
    <footer>
        <p>© 2025 Karan Kumawat - Multimodal Learning Blog</p>
    </footer>
</body>
</html>